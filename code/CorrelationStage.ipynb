{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4ca85a-bfad-47a7-b921-e641bb46c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/tljh/user/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6764ada9-b842-4f53-a69c-9b153b83bc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_649293/3418296366.py:19: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "countBothWays = 0\n",
    "\n",
    "onionAddressData = {}\n",
    "\n",
    "flow_size = 100\n",
    "\n",
    "negetive_samples = 9\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_COUNT = 200\n",
    "learn_rate=0.0001\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "datasetScalerTimes = 1000\n",
    "datasetScalerSizes = 1/1000\n",
    "\n",
    "CNN_21_41 = {'kernelSize1':[2, 30], 'stride1':[2,1], 'kernelSize2':[4, 10], 'stride2':[4,1]} \n",
    "\n",
    "\n",
    "reduce_factor = 16\n",
    "\n",
    "parameters ={\n",
    "    'cnn_secondStage_21_41':CNN_21_41,\n",
    "}\n",
    "    \n",
    "baseDatasetPath = '/home/jupyter-atse/datasets/'\n",
    "\n",
    "metaDataPairsFolders = pickle.load(open(baseDatasetPath + 'metaDataPairsFolders', 'rb'))\n",
    "trainPairsFolders = pickle.load(open(baseDatasetPath + 'trainPairsFolders', 'rb'))\n",
    "testPairsFolders = pickle.load(open(baseDatasetPath + 'testPairsFolders', 'rb'))\n",
    "\n",
    "onionAddressData = metaDataPairsFolders['onionAddressData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea6d835-b306-4b26-82bb-6900fefcab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(pairsFoldersInput):\n",
    "\n",
    "    global negetive_samples, flow_size, onionAddressData, graphTitle\n",
    "\n",
    "    for onionUrl in onionAddressData:\n",
    "        onionAddressData[onionUrl]['connectionIndex'] = 0\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    l2s=np.zeros((len(pairsFoldersInput)*(negetive_samples+1),8,flow_size,1))\n",
    "    labels=np.zeros((len(pairsFoldersInput)*(negetive_samples+1),1))\n",
    "    for pairFolder in pairsFoldersInput:\n",
    "        \n",
    "        clientTimesIn = pairFolder['clientFlow']['timesIn']\n",
    "        clientTimesOut = pairFolder['clientFlow']['timesOut']\n",
    "        clientSizesIn = pairFolder['clientFlow']['sizesIn']\n",
    "        clientSizesOut = pairFolder['clientFlow']['sizesOut']\n",
    "\n",
    "        hsTimesIn = pairFolder['hsFlow']['timesIn']\n",
    "        hsTimesOut = pairFolder['hsFlow']['timesOut']\n",
    "        hsSizesIn = pairFolder['hsFlow']['sizesIn']\n",
    "        hsSizesOut = pairFolder['hsFlow']['sizesOut']\n",
    "\n",
    "\n",
    "        l2s[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "        l2s[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "        labels[index, 0] = 1\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        random = list(range(len(pairsFoldersInput)))\n",
    "        np.random.shuffle(random)\n",
    "        negetive_samples_current = 0\n",
    "        for negetive_pair_index in random:\n",
    "\n",
    "            #skip if is the original correlated pair\n",
    "            if pairsFoldersInput[negetive_pair_index]['hsFolder'] == pairFolder['hsFolder']:\n",
    "                continue\n",
    "\n",
    "            hsTimesIn = pairsFoldersInput[negetive_pair_index]['hsFlow']['timesIn']\n",
    "            hsTimesOut = pairsFoldersInput[negetive_pair_index]['hsFlow']['timesOut']\n",
    "            hsSizesIn = pairsFoldersInput[negetive_pair_index]['hsFlow']['sizesIn']\n",
    "            hsSizesOut = pairsFoldersInput[negetive_pair_index]['hsFlow']['sizesOut']\n",
    "\n",
    "            \n",
    "            l2s[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "            l2s[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "\n",
    "            labels[index, 0] = 0\n",
    "\n",
    "            index += 1\n",
    "\n",
    "            negetive_samples_current += 1\n",
    "            if negetive_samples_current >= negetive_samples:\n",
    "                break\n",
    "\n",
    "        onionAddressData[pairFolder['onionAddress']]['connectionIndex'] += 1\n",
    "\n",
    "    return l2s, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9c62f7-03cb-4668-8dfb-bf7c1105ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 4, 71, 125)        7625      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 4, 71, 125)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 67, 125)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 58, 62)         310062    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1, 58, 62)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 54, 62)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3348)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 187)               626263    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 187)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 187)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                9400      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 306       \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 953,663\n",
      "Trainable params: 953,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:05<05:22,  1.63s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A/opt/tljh/user/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\n",
      "  3%|▎         | 6/200 [00:09<05:06,  1.58s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 9/200 [00:14<05:09,  1.62s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▌         | 12/200 [00:20<05:19,  1.70s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 15/200 [00:24<04:56,  1.60s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 18/200 [00:29<05:02,  1.66s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 21/200 [00:34<04:44,  1.59s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 24/200 [00:38<04:34,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▎        | 27/200 [00:43<04:28,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 30/200 [00:48<04:24,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|█▋        | 33/200 [00:52<04:18,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|█▊        | 36/200 [00:57<04:13,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█▉        | 39/200 [01:02<04:24,  1.64s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 42/200 [01:07<04:11,  1.59s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▎       | 45/200 [01:11<04:01,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 48/200 [01:16<03:55,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 26%|██▌       | 51/200 [01:21<03:50,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 54/200 [01:25<03:46,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 57/200 [01:30<03:44,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|███       | 60/200 [01:36<03:59,  1.71s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 32%|███▏      | 63/200 [01:40<03:39,  1.60s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 66/200 [01:45<03:31,  1.58s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 34%|███▍      | 69/200 [01:50<03:25,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 36%|███▌      | 72/200 [01:54<03:19,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|███▊      | 75/200 [02:00<03:32,  1.70s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 39%|███▉      | 78/200 [02:05<03:33,  1.75s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 81/200 [02:09<03:13,  1.63s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▏     | 84/200 [02:15<03:17,  1.70s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▎     | 87/200 [02:19<03:01,  1.61s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▌     | 90/200 [02:24<02:52,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▋     | 93/200 [02:29<02:50,  1.59s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 48%|████▊     | 96/200 [02:33<02:44,  1.58s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████▉     | 99/200 [02:38<02:37,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 51%|█████     | 102/200 [02:43<02:32,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▎    | 105/200 [02:47<02:27,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 54%|█████▍    | 108/200 [02:52<02:23,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 56%|█████▌    | 111/200 [02:57<02:24,  1.63s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 57%|█████▋    | 114/200 [03:02<02:26,  1.70s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 58%|█████▊    | 117/200 [03:07<02:13,  1.60s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 60%|██████    | 120/200 [03:11<02:05,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 62%|██████▏   | 123/200 [03:16<02:00,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 63%|██████▎   | 126/200 [03:21<01:54,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 64%|██████▍   | 129/200 [03:25<01:50,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 66%|██████▌   | 132/200 [03:30<01:45,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 68%|██████▊   | 135/200 [03:35<01:41,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 69%|██████▉   | 138/200 [03:39<01:36,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 70%|███████   | 141/200 [03:44<01:31,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 72%|███████▏  | 144/200 [03:49<01:27,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 74%|███████▎  | 147/200 [03:54<01:22,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 75%|███████▌  | 150/200 [03:58<01:17,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 76%|███████▋  | 153/200 [04:03<01:13,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 78%|███████▊  | 156/200 [04:08<01:08,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 80%|███████▉  | 159/200 [04:12<01:06,  1.62s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 81%|████████  | 162/200 [04:17<00:59,  1.58s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 82%|████████▎ | 165/200 [04:22<00:56,  1.61s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 84%|████████▍ | 168/200 [04:27<00:50,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 86%|████████▌ | 171/200 [04:32<00:48,  1.67s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 87%|████████▋ | 174/200 [04:37<00:41,  1.60s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 88%|████████▊ | 177/200 [04:41<00:36,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 90%|█████████ | 180/200 [04:46<00:31,  1.59s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 92%|█████████▏| 183/200 [04:51<00:26,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 93%|█████████▎| 186/200 [04:55<00:21,  1.56s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 94%|█████████▍| 189/200 [05:00<00:17,  1.55s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 96%|█████████▌| 192/200 [05:05<00:12,  1.61s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 98%|█████████▊| 195/200 [05:10<00:07,  1.57s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 99%|█████████▉| 198/200 [05:14<00:03,  1.59s/it]\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 200/200 [05:18<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "for modelParametersKey in parameters:\n",
    "    \n",
    "    #create directory for models\n",
    "    if not os.path.exists(modelParametersKey):\n",
    "        os.makedirs(modelParametersKey)\n",
    "\n",
    "\n",
    "    #model definition\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    orig = [2000/reduce_factor, 1000/reduce_factor, 49600/reduce_factor, 3000/reduce_factor, 800/reduce_factor, 100/reduce_factor]\n",
    "    dropout_prob = 0.4\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(orig[0]), input_shape=[8, flow_size, 1], kernel_size=parameters[modelParametersKey]['kernelSize1'], strides=parameters[modelParametersKey]['stride1'], padding='VALID', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(1, 5), strides=(1, 1), padding='VALID'))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(orig[1]), kernel_size=parameters[modelParametersKey]['kernelSize2'], strides=parameters[modelParametersKey]['stride2'], padding='VALID', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(1, 5), strides=(1, 1), padding='VALID')) \n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(int(orig[3]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(int(orig[4]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(int(orig[5]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    def customLoss(y_true,y_pred):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=y_true),name='loss_sigmoid')\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=customLoss, optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
    "    # custom function\n",
    "    def sigmoid(x):\n",
    "        if x < -150:\n",
    "            x = -150\n",
    "        if x > 150:\n",
    "            x = 150\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    # define vectorized sigmoid\n",
    "    sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "\n",
    "\n",
    "    losses = [10, 10]\n",
    "    saveNames = ['', '']\n",
    "    precisionBest = 0\n",
    "    saveNamePrecisionBest = ''\n",
    "    f1_05 = 0\n",
    "    saveNameF1_05 = ''\n",
    "\n",
    "    correlatedPerRound = 10\n",
    "\n",
    "\n",
    "    tpBest = 0\n",
    "    tp = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    fpBest = 0\n",
    "    fp = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    tn = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    fn = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(EPOCH_COUNT)):\n",
    "            l2s, labels = generateDataset(trainPairsFolders)\n",
    "            rr= list(range(len(l2s)))\n",
    "            np.random.shuffle(rr)\n",
    "            l2s=l2s[rr]\n",
    "            labels=labels[rr]\n",
    "            \n",
    "            history = model.fit(l2s, labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "            loss = history.history['loss'][-1]\n",
    "            \n",
    "\n",
    "            path = 'epoch' + str(epoch) + '_loss' + str(loss) + '/cp.ckpt'\n",
    "            model.save_weights(modelParametersKey + '/' + 'last_' + path)\n",
    "            for filename in glob.glob(modelParametersKey + '/' + 'last_' + 'epoch' + str(epoch-1) + '*'):\n",
    "                shutil.rmtree(filename)\n",
    "\n",
    "            #save model based on precision\n",
    "            if epoch % 3 == 0 and epoch > 0:\n",
    "                for onionUrl in onionAddressData:\n",
    "                    onionAddressData[onionUrl]['connectionIndex'] = 0\n",
    "\n",
    "                for i in tqdm.tqdm(range(len(testPairsFolders)//correlatedPerRound-1)):\n",
    "                    l2s_test=np.zeros(((negetive_samples+1)*correlatedPerRound,8,flow_size,1))\n",
    "                    labels_test=np.zeros(((negetive_samples+1)*correlatedPerRound))\n",
    "\n",
    "                    index = 0\n",
    "\n",
    "                    for pairFolder in testPairsFolders[i*correlatedPerRound:(i+1)*correlatedPerRound]:\n",
    "\n",
    "                        clientTimesIn = pairFolder['clientFlow']['timesIn']\n",
    "                        clientTimesOut = pairFolder['clientFlow']['timesOut']\n",
    "                        clientSizesIn = pairFolder['clientFlow']['sizesIn']\n",
    "                        clientSizesOut = pairFolder['clientFlow']['sizesOut']\n",
    "\n",
    "                        hsTimesIn = pairFolder['hsFlow']['timesIn']\n",
    "                        hsTimesOut = pairFolder['hsFlow']['timesOut']\n",
    "                        hsSizesIn = pairFolder['hsFlow']['sizesIn']\n",
    "                        hsSizesOut = pairFolder['hsFlow']['sizesOut']                  \n",
    "\n",
    "                        l2s_test[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "                        l2s_test[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "                        labels_test[index] = 1\n",
    "\n",
    "                        index += 1\n",
    "\n",
    "                        random = list(range(len(testPairsFolders)))\n",
    "                        np.random.shuffle(random)\n",
    "                        negetive_samples_current = 0\n",
    "                        for negetive_pair_index in random:\n",
    "\n",
    "                            #if is the corresponding \n",
    "                            if testPairsFolders[negetive_pair_index]['hsFolder'] == pairFolder['hsFolder']:\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            hsTimesIn = testPairsFolders[negetive_pair_index]['hsFlow']['timesIn']\n",
    "                            hsTimesOut = testPairsFolders[negetive_pair_index]['hsFlow']['timesOut']\n",
    "                            hsSizesIn = testPairsFolders[negetive_pair_index]['hsFlow']['sizesIn']\n",
    "                            hsSizesOut = testPairsFolders[negetive_pair_index]['hsFlow']['sizesOut']   \n",
    "\n",
    "\n",
    "                            l2s_test[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "                            l2s_test[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "\n",
    "                            labels_test[index] = 0\n",
    "\n",
    "                            index += 1\n",
    "\n",
    "                            negetive_samples_current += 1\n",
    "                            if negetive_samples_current >= negetive_samples:\n",
    "                                break\n",
    "\n",
    "                    #ignore batch size and use batches of negetive_samples+1\n",
    "                    prediction = model.predict(l2s_test)\n",
    "                    break\n",
    "                    results = sigmoid_v(prediction).tolist()\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7c307b-83c2-478b-9dc1-81aa092d2648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.9109665\n",
      "Execution time =  0.2109842300415039\n"
     ]
    }
   ],
   "source": [
    "this_execution = np.zeros(10)\n",
    "\n",
    "l3s, labels3 = generateDataset(testPairsFolders)\n",
    "for j in range(10):\n",
    "    start_test = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(l3s,labels3, batch_size=BATCH_SIZE, verbose=1)\n",
    "    end_test = time.time()\n",
    "    this_execution[j] = end_test - start_test\n",
    "print('Accuracy on test dataset:', test_accuracy)\n",
    "print('Execution time = ', np.min(this_execution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755aa8aa-0dbd-4306-b4cd-7053479180f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
