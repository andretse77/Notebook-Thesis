{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dd29e4-cdfa-434d-858b-7a89e9613432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/tljh/user/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07744784-1f41-4f7c-a1b3-65ebc227c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_3009626/3418296366.py:19: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "countBothWays = 0\n",
    "\n",
    "onionAddressData = {}\n",
    "\n",
    "flow_size = 100\n",
    "\n",
    "negetive_samples = 9\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_COUNT = 200\n",
    "learn_rate=0.0001\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "datasetScalerTimes = 1000\n",
    "datasetScalerSizes = 1/1000\n",
    "\n",
    "CNN_21_41 = {'kernelSize1':[2, 30], 'stride1':[2,1], 'kernelSize2':[4, 10], 'stride2':[4,1]} \n",
    "\n",
    "reduce_factor = 16\n",
    "\n",
    "parameters ={\n",
    "    'cnn_secondStage_21_41':CNN_21_41,\n",
    "}\n",
    "    \n",
    "baseDatasetPath = '/home/jupyter-atse/datasets/'\n",
    "\n",
    "metaDataPairsFolders = pickle.load(open(baseDatasetPath + 'metaDataPairsFolders', 'rb'))\n",
    "trainPairsFolders = pickle.load(open(baseDatasetPath + 'trainPairsFolders', 'rb'))\n",
    "testPairsFolders = pickle.load(open(baseDatasetPath + 'testPairsFolders', 'rb'))\n",
    "\n",
    "onionAddressData = metaDataPairsFolders['onionAddressData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee6c57-de9e-4943-8ee3-ce2f4a8a803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(pairsFoldersInput):\n",
    "\n",
    "    global negetive_samples, flow_size, onionAddressData, graphTitle\n",
    "\n",
    "    for onionUrl in onionAddressData:\n",
    "        onionAddressData[onionUrl]['connectionIndex'] = 0\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    l2s=np.zeros((len(pairsFoldersInput)*(negetive_samples+1),8,flow_size,1))\n",
    "    labels=np.zeros((len(pairsFoldersInput)*(negetive_samples+1),1))\n",
    "    for pairFolder in pairsFoldersInput:\n",
    "        \n",
    "        clientTimesIn = pairFolder['clientFlow']['timesIn']\n",
    "        clientTimesOut = pairFolder['clientFlow']['timesOut']\n",
    "        clientSizesIn = pairFolder['clientFlow']['sizesIn']\n",
    "        clientSizesOut = pairFolder['clientFlow']['sizesOut']\n",
    "\n",
    "        hsTimesIn = pairFolder['hsFlow']['timesIn']\n",
    "        hsTimesOut = pairFolder['hsFlow']['timesOut']\n",
    "        hsSizesIn = pairFolder['hsFlow']['sizesIn']\n",
    "        hsSizesOut = pairFolder['hsFlow']['sizesOut']\n",
    "\n",
    "\n",
    "        l2s[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "        l2s[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "        l2s[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "        l2s[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "        labels[index, 0] = 1\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        random = list(range(len(pairsFoldersInput)))\n",
    "        np.random.shuffle(random)\n",
    "        negetive_samples_current = 0\n",
    "        for negetive_pair_index in random:\n",
    "\n",
    "            #skip if is the original correlated pair\n",
    "            if pairsFoldersInput[negetive_pair_index]['hsFolder'] == pairFolder['hsFolder']:\n",
    "                continue\n",
    "\n",
    "            hsTimesIn = pairsFoldersInput[negetive_pair_index]['hsFlow']['timesIn']\n",
    "            hsTimesOut = pairsFoldersInput[negetive_pair_index]['hsFlow']['timesOut']\n",
    "            hsSizesIn = pairsFoldersInput[negetive_pair_index]['hsFlow']['sizesIn']\n",
    "            hsSizesOut = pairsFoldersInput[negetive_pair_index]['hsFlow']['sizesOut']\n",
    "\n",
    "            \n",
    "            l2s[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "            l2s[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "            l2s[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "            l2s[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "\n",
    "            labels[index, 0] = 0\n",
    "\n",
    "            index += 1\n",
    "\n",
    "            negetive_samples_current += 1\n",
    "            if negetive_samples_current >= negetive_samples:\n",
    "                break\n",
    "\n",
    "        onionAddressData[pairFolder['onionAddress']]['connectionIndex'] += 1\n",
    "\n",
    "    #print(onionAddressData)\n",
    "\n",
    "    return l2s, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2876525-61b7-45d4-835c-bf3c49673c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelParametersKey in parameters:\n",
    "    \n",
    "    #create directory for models\n",
    "    if not os.path.exists(modelParametersKey):\n",
    "        os.makedirs(modelParametersKey)\n",
    "\n",
    "\n",
    "    #model definition\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    orig = [2000/reduce_factor, 1000/reduce_factor, 500/reduce_factor, 3000/reduce_factor, 800/reduce_factor, 100/reduce_factor]\n",
    "    dropout_prob = 0.4\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(orig[0]), input_shape=[8, flow_size, 1], kernel_size=[2,30], strides=parameters[modelParametersKey]['stride1'], padding='VALID', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(1, 5), strides=(1, 1), padding='VALID'))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(orig[1]), kernel_size=[2,30], strides=parameters[modelParametersKey]['stride1'], padding='VALID', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(1, 5), strides=(1, 1), padding='VALID'))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(int(orig[2]), kernel_size=[2,30], strides=parameters[modelParametersKey]['stride2'], padding='VALID', kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(1, 5), strides=(1, 1), padding='VALID')) \n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(int(orig[3]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(int(orig[4]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(int(orig[5]), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    def customLoss(y_true,y_pred):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=y_true),name='loss_sigmoid')\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=customLoss, optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
    "    # custom function\n",
    "    def sigmoid(x):\n",
    "        if x < -150:\n",
    "            x = -150\n",
    "        if x > 150:\n",
    "            x = 150\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    # define vectorized sigmoid\n",
    "    sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "\n",
    "\n",
    "    losses = [10, 10]\n",
    "    saveNames = ['', '']\n",
    "    precisionBest = 0\n",
    "    saveNamePrecisionBest = ''\n",
    "    f1_05 = 0\n",
    "    saveNameF1_05 = ''\n",
    "\n",
    "    correlatedPerRound = 10\n",
    "\n",
    "\n",
    "    tpBest = 0\n",
    "    tp = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    fpBest = 0\n",
    "    fp = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    tn = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "    fn = { 0.01 : 0, 0.05 : 0, 0.1 : 0, 0.2 : 0, 0.3 : 0, 0.4 : 0, 0.5 : 0, 0.6 : 0, 0.7 : 0, 0.8 : 0, 0.9 : 0, 0.95 : 0, 0.99 : 0}\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(EPOCH_COUNT)):\n",
    "            l2s, labels = generateDataset(trainPairsFolders)\n",
    "            rr= list(range(len(l2s)))\n",
    "            np.random.shuffle(rr)\n",
    "            l2s=l2s[rr]\n",
    "            labels=labels[rr]\n",
    "            \n",
    "            history = model.fit(l2s, labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "            loss = history.history['loss'][-1]\n",
    "            #print(\"Epoch:\", epoch, \"loss:\", loss)\n",
    "            \n",
    "\n",
    "            path = 'epoch' + str(epoch) + '_loss' + str(loss) + '/cp.ckpt'\n",
    "            model.save_weights(modelParametersKey + '/' + 'last_' + path)\n",
    "            for filename in glob.glob(modelParametersKey + '/' + 'last_' + 'epoch' + str(epoch-1) + '*'):\n",
    "                shutil.rmtree(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #save model based on precision\n",
    "            if epoch % 3 == 0 and epoch > 0:\n",
    "                for onionUrl in onionAddressData:\n",
    "                    onionAddressData[onionUrl]['connectionIndex'] = 0\n",
    "\n",
    "                for i in tqdm.tqdm(range(len(testPairsFolders)//correlatedPerRound-1)):\n",
    "                    l2s_test=np.zeros(((negetive_samples+1)*correlatedPerRound,8,flow_size,1))\n",
    "                    labels_test=np.zeros(((negetive_samples+1)*correlatedPerRound))\n",
    "\n",
    "                    index = 0\n",
    "\n",
    "                    for pairFolder in testPairsFolders[i*correlatedPerRound:(i+1)*correlatedPerRound]:\n",
    "\n",
    "                        clientTimesIn = pairFolder['clientFlow']['timesIn']\n",
    "                        clientTimesOut = pairFolder['clientFlow']['timesOut']\n",
    "                        clientSizesIn = pairFolder['clientFlow']['sizesIn']\n",
    "                        clientSizesOut = pairFolder['clientFlow']['sizesOut']\n",
    "\n",
    "                        hsTimesIn = pairFolder['hsFlow']['timesIn']\n",
    "                        hsTimesOut = pairFolder['hsFlow']['timesOut']\n",
    "                        hsSizesIn = pairFolder['hsFlow']['sizesIn']\n",
    "                        hsSizesOut = pairFolder['hsFlow']['sizesOut']                  \n",
    "\n",
    "                        l2s_test[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "                        l2s_test[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "                        l2s_test[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "                        l2s_test[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "                        labels_test[index] = 1\n",
    "\n",
    "                        index += 1\n",
    "\n",
    "                        random = list(range(len(testPairsFolders)))\n",
    "                        np.random.shuffle(random)\n",
    "                        negetive_samples_current = 0\n",
    "                        for negetive_pair_index in random:\n",
    "\n",
    "                            #if is the corresponding \n",
    "                            if testPairsFolders[negetive_pair_index]['hsFolder'] == pairFolder['hsFolder']:\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            hsTimesIn = testPairsFolders[negetive_pair_index]['hsFlow']['timesIn']\n",
    "                            hsTimesOut = testPairsFolders[negetive_pair_index]['hsFlow']['timesOut']\n",
    "                            hsSizesIn = testPairsFolders[negetive_pair_index]['hsFlow']['sizesIn']\n",
    "                            hsSizesOut = testPairsFolders[negetive_pair_index]['hsFlow']['sizesOut']   \n",
    "\n",
    "\n",
    "                            l2s_test[index,0,:len(clientTimesIn[:flow_size]),0]=np.array(clientTimesIn[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,1,:len(hsTimesOut[:flow_size]),0]=np.array(hsTimesOut[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,2,:len(hsTimesIn[:flow_size]),0]=np.array(hsTimesIn[:flow_size])*datasetScalerTimes\n",
    "                            l2s_test[index,3,:len(clientTimesOut[:flow_size]),0]=np.array(clientTimesOut[:flow_size])*datasetScalerTimes\n",
    "\n",
    "                            l2s_test[index,4,:len(clientSizesIn[:flow_size]),0]=np.array(clientSizesIn[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,5,:len(hsSizesOut[:flow_size]),0]=np.array(hsSizesOut[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,6,:len(hsSizesIn[:flow_size]),0]=np.array(hsSizesIn[:flow_size])*datasetScalerSizes\n",
    "                            l2s_test[index,7,:len(clientSizesOut[:flow_size]),0]=np.array(clientSizesOut[:flow_size])*datasetScalerSizes\n",
    "\n",
    "\n",
    "                            labels_test[index] = 0\n",
    "\n",
    "                            index += 1\n",
    "\n",
    "                            negetive_samples_current += 1\n",
    "                            if negetive_samples_current >= negetive_samples:\n",
    "                                break\n",
    "\n",
    "                    #ignore batch size and use batches of negetive_samples+1\n",
    "                    prediction = model.predict(l2s_test)\n",
    "                    break\n",
    "                    results = sigmoid_v(prediction).tolist()\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366507de-c035-4a4c-addf-70ef5ae1e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.9\n",
      "Execution time =  0.26430392265319824\n"
     ]
    }
   ],
   "source": [
    "this_execution = np.zeros(10)\n",
    "\n",
    "l3s, labels3 = generateDataset(testPairsFolders)\n",
    "for j in range(10):\n",
    "    start_test = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(l3s,labels3, batch_size=BATCH_SIZE, verbose=1)\n",
    "    end_test = time.time()\n",
    "    this_execution[j] = end_test - start_test\n",
    "print('Accuracy on test dataset:', test_accuracy)\n",
    "print('Execution time = ', np.min(this_execution))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
